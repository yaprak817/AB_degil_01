# ============================================================
# FULL-FEATURE CATBOOST TRAIN (Hackathon iÃ§in Ã¶nerilen)
# + Smoothed TE (District/Neighborhood)
# + UI bundle: expected_features, default_row, ui_cols
# Output: models/model_bundle.pkl
# ============================================================

import os, re, glob, warnings
import numpy as np
import pandas as pd
warnings.filterwarnings("ignore")

from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_percentage_error

from catboost import CatBoostRegressor, Pool
import joblib

SEED = 42
eps = 1e-9

# ---------------------------
# 0) CSV bul / oku
# ---------------------------
WIN_DEFAULT = r"C:\Users\Lenovo\Desktop\hackathon_train_set.csv"

def robust_read_csv(path: str) -> pd.DataFrame:
    attempts = [
        dict(sep=";", encoding="utf-8"),
        dict(sep=";", encoding="latin-1"),
        dict(sep=",", encoding="utf-8"),
        dict(sep=",", encoding="latin-1"),
        dict(sep=None, engine="python", encoding="utf-8"),
        dict(sep=None, engine="python", encoding="latin-1"),
    ]
    last = None
    for kw in attempts:
        try:
            df_ = pd.read_csv(path, **kw)
            if df_.shape[1] == 1:
                c0 = df_.columns[0]
                sample = df_[c0].astype(str).head(20).str.cat(sep=" ")
                if (";" in sample) or ("," in sample):
                    continue
            return df_
        except Exception as e:
            last = e
    raise RuntimeError(f"CSV okunamadÄ±. Son hata: {last}")

def resolve_csv():
    if os.path.exists(WIN_DEFAULT):
        return WIN_DEFAULT
    csvs = glob.glob("*.csv")
    if csvs:
        return csvs[0]
    raise RuntimeError("CSV bulunamadÄ±. WIN_DEFAULT yolunu dÃ¼zelt veya dosyayÄ± bu klasÃ¶re koy.")

csv_path = resolve_csv()
df = robust_read_csv(csv_path)
df.columns = df.columns.str.strip()
print("âœ… CSV:", csv_path, "| shape:", df.shape)

# ---------------------------
# 1) Target (Price) bul/temizle
# ---------------------------
def clean_price(p):
    if pd.isna(p): return np.nan
    s = str(p).replace("TL","").replace("tl","").replace("â‚º","").strip()
    s = re.sub(r"[^0-9\.,]", "", s)
    if "." in s and "," in s:
        s = s.replace(".","").replace(",",".")
    else:
        if "," in s and len(s.split(",")[-1]) == 3:
            s = s.replace(",","")
        if "." in s and len(s.split(".")[-1]) == 3:
            s = s.replace(".","")
        s = s.replace(",",".")
    try:
        return float(s)
    except:
        return np.nan

price_candidates = [c for c in df.columns if ("price" in c.lower()) or ("fiyat" in c.lower())]
if not price_candidates:
    raise ValueError("âŒ Price/Fiyat sÃ¼tunu bulunamadÄ±.")
TARGET = price_candidates[0]
df[TARGET] = df[TARGET].apply(clean_price)
df = df.dropna(subset=[TARGET]).copy()

# loan filtresi
loan_cols = [c for c in df.columns if ("available for loan" in c.lower()) or ("loan" in c.lower()) or ("kredi" in c.lower())]
if loan_cols:
    lc = loan_cols[0]
    before = len(df)
    df = df[df[lc].astype(str).str.contains("yes|evet|true", case=False, na=False)].copy()
    print(f"ðŸ¦ Loan filtresi ({lc}) Ã§Ä±karÄ±lan:", before - len(df))

# sabit makul fiyat aralÄ±ÄŸÄ± (quantile clip yok)
df = df[(df[TARGET] >= 100_000) & (df[TARGET] <= 50_000_000)].copy()

# ---------------------------
# 2) Parse yardÄ±mcÄ±larÄ±
# ---------------------------
def clean_numeric_series(s: pd.Series) -> pd.Series:
    ss = s.astype(str).str.replace(",", ".", regex=False)
    ss = ss.str.replace(r"[^0-9\.]", "", regex=True)
    return pd.to_numeric(ss, errors="coerce")

def parse_rooms(val):
    if pd.isna(val): return np.nan
    t = str(val).lower().strip()
    if "studio" in t or "1+0" in t:
        return 1.0
    nums = re.findall(r"\d+", t)
    if len(nums) >= 2 and "+" in t:
        return float(nums[0]) + float(nums[1])
    if len(nums) == 1:
        return float(nums[0])
    return np.nan

def parse_age(val):
    if pd.isna(val): return np.nan
    t = str(val).lower().strip()
    if "more" in t or ">" in t:
        return 30.0
    nums = re.findall(r"\d+", t)
    if len(nums) == 2:
        return (float(nums[0]) + float(nums[1])) / 2
    if len(nums) == 1:
        return float(nums[0])
    return np.nan

# KolonlarÄ± bul (UI iÃ§in de kullanacaÄŸÄ±z)
def find_col(cands):
    cols = df.columns.tolist()
    low = {c.lower(): c for c in cols}
    for cand in cands:
        for k, orig in low.items():
            if cand in k:
                return orig
    return None

UI_COLS = {
    "district": find_col(["district", "ilce", "ilÃ§e"]),
    "neighborhood": find_col(["neighborhood", "mahalle", "mah"]),
    "area": find_col(["mÂ² (gross)", "gross", "net area", "mÂ²", "m2", "area", "square"]),
    "rooms": find_col(["number of rooms", "rooms"]),
    "baths": find_col(["number of bathrooms", "bath", "banyo"]),
    "age": find_col(["building age", "age", "yas", "yasi"]),
}
print("ðŸ§© UI_COLS:", UI_COLS)

# Rooms/Age parse uygula (kolon varsa)
if UI_COLS["rooms"] and UI_COLS["rooms"] in df.columns:
    df[UI_COLS["rooms"]] = df[UI_COLS["rooms"]].apply(parse_rooms)
if UI_COLS["age"] and UI_COLS["age"] in df.columns:
    df[UI_COLS["age"]] = df[UI_COLS["age"]].apply(parse_age)

# m2/baths numeric'e Ã§ek (varsa)
if UI_COLS["area"] and UI_COLS["area"] in df.columns:
    df[UI_COLS["area"]] = clean_numeric_series(df[UI_COLS["area"]])
if UI_COLS["baths"] and UI_COLS["baths"] in df.columns:
    df[UI_COLS["baths"]] = clean_numeric_series(df[UI_COLS["baths"]])

# ---------------------------
# 3) Feature engineering (gÃ¼venli)
# ---------------------------
# Luxury score: kolon adÄ± iÃ§inde geÃ§enlere gÃ¶re varsa topla
lux_keywords = ["pool", "jacuzzi", "security", "elevator", "garage"]
lux_cols = [c for c in df.columns if any(k in c.lower() for k in lux_keywords)]
if lux_cols:
    # bool/string olabilir -> sayÄ±sala Ã§evirmeye Ã§alÄ±ÅŸ
    tmp = df[lux_cols].copy()
    for c in lux_cols:
        if tmp[c].dtype == "object":
            tmp[c] = tmp[c].astype(str).str.contains("yes|evet|true|1", case=False, na=False).astype(int)
    df["luxury_score"] = tmp.sum(axis=1)
else:
    df["luxury_score"] = 0

# Wealth score (District varsa)
ses_tiers = {
    'BeÅŸiktaÅŸ': 5, 'KadÄ±kÃ¶y': 5, 'BakÄ±rkÃ¶y': 5, 'ÅžiÅŸli': 5, 'SarÄ±yer': 5, 'Adalar': 5,
    'ÃœskÃ¼dar': 4, 'AtaÅŸehir': 4, 'Maltepe': 4, 'BeyoÄŸlu': 4, 'Beykoz': 4,
    'Fatih': 3, 'BahÃ§elievler': 3, 'Zeytinburnu': 3, 'Kartal': 3, 'KÃ¼Ã§Ã¼kÃ§ekmece': 3, 'BaÅŸakÅŸehir': 3, 'EyÃ¼psultan': 3,
    'Ãœmraniye': 2, 'KaÄŸÄ±thane': 2, 'Pendik': 2, 'Tuzla': 2, 'BÃ¼yÃ¼kÃ§ekmece': 2, 'Ã‡ekmekÃ¶y': 2, 'BeylikdÃ¼zÃ¼': 2, 'AvcÄ±lar': 2, 'BayrampaÅŸa': 2,
    'Esenyurt': 1, 'ArnavutkÃ¶y': 1, 'Sultanbeyli': 1, 'Sultangazi': 1, 'BaÄŸcÄ±lar': 1, 'Esenler': 1, 'GÃ¼ngÃ¶ren': 1, 'GaziosmanpaÅŸa': 1, 'Ã‡atalca': 1, 'Åžile': 1, 'Silivri': 1, 'Sancaktepe': 1
}
dist_col = UI_COLS["district"]
if dist_col and dist_col in df.columns:
    df["wealth_score"] = df[dist_col].astype(str).map(ses_tiers).fillna(2).astype(int)
else:
    df["wealth_score"] = 2

# Alan bazlÄ± FE (varsa)
area_col = UI_COLS["area"]
rooms_col = UI_COLS["rooms"]
baths_col = UI_COLS["baths"]
age_col = UI_COLS["age"]

if area_col and area_col in df.columns:
    df["log_area"] = np.log1p(df[area_col].fillna(df[area_col].median()).clip(lower=0))
if area_col and rooms_col and area_col in df.columns and rooms_col in df.columns:
    df["area_per_room"] = df[area_col] / (df[rooms_col] + 1.0)
if rooms_col and baths_col and rooms_col in df.columns and baths_col in df.columns:
    df["room_bath_ratio"] = df[rooms_col] / (df[baths_col] + 1e-3)
if age_col and age_col in df.columns:
    df["age_bucket"] = pd.cut(df[age_col], bins=[-1,5,15,30,200], labels=["0-5","6-15","16-30","30+"]).astype(str)

# ---------------------------
# 4) Leakage guard drop
# ---------------------------
leak_words = ["price","fiyat","tl","â‚º","ucret","Ã¼cret","bedel"]
auto_leak = [c for c in df.columns if c != TARGET and any(w in c.lower() for w in leak_words)]
drop_like = ["id","listing_id","ad number","url","image","images","title","description","phone"]
manual_drop = [c for c in df.columns if any(k in c.lower() for k in drop_like)]

drop_cols = sorted(set([TARGET] + auto_leak + manual_drop + loan_cols))
X = df.drop(columns=drop_cols, errors="ignore").copy()
y = df[TARGET].astype(float).copy()
y_log = np.log1p(y)

# kategorikler
cat_cols = X.select_dtypes(include=["object","category"]).columns.tolist()
for c in cat_cols:
    X[c] = X[c].fillna("Unknown").astype(str)

# numerikler
num_cols = [c for c in X.columns if c not in cat_cols]
for c in num_cols:
    X[c] = X[c].fillna(X[c].median())

print("âœ… Features:", X.shape[1], "| cat:", len(cat_cols))

# ---------------------------
# 5) Smoothed TE (District/Neighborhood) - train -> valid/test
# ---------------------------
def te_fit(train_key: pd.Series, train_ylog: pd.Series, k=30):
    global_mean = float(train_ylog.mean())
    global_med  = float(np.median(train_ylog.values))
    grp = pd.DataFrame({"k": train_key.astype(str), "y": train_ylog.values})
    agg = grp.groupby("k")["y"].agg(["count","mean","median"])
    agg["sm_mean"] = (agg["count"]*agg["mean"] + k*global_mean) / (agg["count"] + k)
    agg["sm_med"]  = (agg["count"]*agg["median"] + k*global_med) / (agg["count"] + k)
    return {
        "mean": agg["sm_mean"].to_dict(),
        "med":  agg["sm_med"].to_dict(),
        "cnt":  agg["count"].to_dict(),
        "global_mean": global_mean,
        "global_med": global_med,
        "k": k
    }

def te_apply(key: pd.Series, maps, prefix: str, colname: str) -> pd.DataFrame:
    k = key.astype(str)
    out = pd.DataFrame(index=key.index)
    out[f"{prefix}_{colname}_mean"] = k.map(maps["mean"]).fillna(maps["global_mean"]).astype(float)
    out[f"{prefix}_{colname}_med"]  = k.map(maps["med"]).fillna(maps["global_med"]).astype(float)
    out[f"{prefix}_{colname}_cnt"]  = k.map(maps["cnt"]).fillna(0).astype(float)
    return out

# split: train/valid/test
X_trval, X_test, y_trval, y_test, ylog_trval, ylog_test = train_test_split(
    X, y, y_log, test_size=0.20, random_state=SEED
)
X_train, X_valid, y_train, y_valid, ylog_train, ylog_valid = train_test_split(
    X_trval, y_trval, ylog_trval, test_size=0.20, random_state=SEED
)

te_maps_bundle = {}

# District TE
if dist_col and dist_col in X_train.columns:
    maps_d = te_fit(X_train[dist_col], ylog_train, k=30)
    te_maps_bundle["district"] = {"col": dist_col, "maps": maps_d}
    X_train = pd.concat([X_train, te_apply(X_train[dist_col], maps_d, "te", dist_col)], axis=1)
    X_valid = pd.concat([X_valid, te_apply(X_valid[dist_col], maps_d, "te", dist_col)], axis=1)
    X_test  = pd.concat([X_test,  te_apply(X_test[dist_col],  maps_d, "te", dist_col)], axis=1)

# Neighborhood TE
neigh_col = UI_COLS["neighborhood"]
if neigh_col and neigh_col in X_train.columns:
    maps_n = te_fit(X_train[neigh_col], ylog_train, k=50)
    te_maps_bundle["neighborhood"] = {"col": neigh_col, "maps": maps_n}
    X_train = pd.concat([X_train, te_apply(X_train[neigh_col], maps_n, "te", neigh_col)], axis=1)
    X_valid = pd.concat([X_valid, te_apply(X_valid[neigh_col], maps_n, "te", neigh_col)], axis=1)
    X_test  = pd.concat([X_test,  te_apply(X_test[neigh_col],  maps_n, "te", neigh_col)], axis=1)

# update cat/num after TE
cat_cols2 = X_train.select_dtypes(include=["object","category"]).columns.tolist()
for c in cat_cols2:
    X_train[c] = X_train[c].fillna("Unknown").astype(str)
    X_valid[c] = X_valid[c].fillna("Unknown").astype(str)
    X_test[c]  = X_test[c].fillna("Unknown").astype(str)

num_cols2 = [c for c in X_train.columns if c not in cat_cols2]
for c in num_cols2:
    medv = X_train[c].median()
    X_train[c] = X_train[c].fillna(medv)
    X_valid[c] = X_valid[c].fillna(medv)
    X_test[c]  = X_test[c].fillna(medv)

# ---------------------------
# 6) Train CatBoost (early stopping)
# ---------------------------
params = dict(
    loss_function="RMSE",
    eval_metric="RMSE",
    iterations=8000,
    learning_rate=0.05,
    depth=8,
    l2_leaf_reg=6,
    random_strength=1.2,
    bagging_temperature=0.8,
    rsm=0.9,
    random_seed=SEED,
    verbose=300
)

train_pool = Pool(X_train, ylog_train, cat_features=cat_cols2)
valid_pool = Pool(X_valid, ylog_valid, cat_features=cat_cols2)

model = CatBoostRegressor(**params)
model.fit(train_pool, eval_set=valid_pool, use_best_model=True, early_stopping_rounds=300)

best_iter = model.get_best_iteration()
print("âœ… BestIter:", best_iter)

def eval_real(m, X_, y_):
    pred = np.expm1(m.predict(X_))
    rmse = float(np.sqrt(mean_squared_error(y_, pred)))
    r2 = float(r2_score(y_, pred))
    mape = float(mean_absolute_percentage_error(y_.values + eps, pred + eps) * 100)
    return r2, rmse, mape

tr_m = eval_real(model, X_train, y_train)
va_m = eval_real(model, X_valid, y_valid)
te_m = eval_real(model, X_test,  y_test)

print("\nðŸ“Š FULL MODEL PERF (log target)")
print(f"Train: R2={tr_m[0]:.4f} | RMSE={tr_m[1]:,.0f} | MAPE={tr_m[2]:.2f}%")
print(f"Valid: R2={va_m[0]:.4f} | RMSE={va_m[1]:,.0f} | MAPE={va_m[2]:.2f}%")
print(f"Test : R2={te_m[0]:.4f} | RMSE={te_m[1]:,.0f} | MAPE={te_m[2]:.2f}%")

# ---------------------------
# 7) Bundle iÃ§in expected_features + default_row
#    (modelin beklediÄŸi X_train kolon seti)
# ---------------------------
expected_features = X_train.columns.tolist()

# default_row: train median/mode
default_row = {}
for c in expected_features:
    if c in cat_cols2:
        m = X_train[c].mode()
        default_row[c] = (m.iloc[0] if len(m) else "Unknown")
    else:
        default_row[c] = float(X_train[c].median())

# robust threshold (MAD) train residuals
train_pred = np.expm1(model.predict(X_train))
pct_err = (y_train.values - train_pred) / (y_train.values + eps) * 100
med = np.median(pct_err)
mad = np.median(np.abs(pct_err - med))
threshold_pct = float(1.4826 * mad)

# save
os.makedirs("models", exist_ok=True)
bundle = {
    "model_type": "catboost_full",
    "model": model,
    "target_col": TARGET,
    "drop_cols": drop_cols,
    "cat_cols": cat_cols2,
    "num_cols": [c for c in expected_features if c not in cat_cols2],
    "te_maps": te_maps_bundle,
    "expected_features": expected_features,
    "default_row": default_row,
    "ui_cols": UI_COLS,
    "threshold_pct": threshold_pct,
    "metrics": {
        "train": {"r2": tr_m[0], "rmse": tr_m[1], "mape": tr_m[2]},
        "valid": {"r2": va_m[0], "rmse": va_m[1], "mape": va_m[2]},
        "test":  {"r2": te_m[0], "rmse": te_m[1], "mape": te_m[2]},
        "best_iter": int(best_iter) if best_iter is not None else None
    }
}

out_path = os.path.join("models", "model_bundle.pkl")
joblib.dump(bundle, out_path)

print(f"\nðŸŽ¯ Threshold: Â±{threshold_pct:.2f}%")
print("âœ… Saved:", out_path)
print("âœ… Feature count:", len(expected_features))

